{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import random\n",
    "import itertools\n",
    "import json\n",
    "import import_ipynb\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "from scipy.io import wavfile\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.layers import Input, Dense,Conv1D, MaxPooling1D, UpSampling1D, Flatten, Reshape\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import get_custom_objects\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "from tensorflow.python.ops import math_ops\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################\n",
    "# utility functions\n",
    "# def cut_old(arr, length):\n",
    "#   idx = len(arr)%length\n",
    "#   out = []\n",
    "#   while(idx+length <= len(arr)):\n",
    "#     out.append(arr[idx:idx+length])\n",
    "#     idx += length\n",
    "#   return np.array(out)\n",
    "\n",
    "def cut(arr, length):\n",
    "  ueberhang = len(arr) % length\n",
    "  a = np.array(arr[ueberhang:]).reshape(-1, length)\n",
    "  return a\n",
    "\n",
    "# def loadSong(fName, numTotalSongs = 1):\n",
    "#   fs, data = wavfile.read(inpathTrain + fName)\n",
    "#   all_data = [data]\n",
    "\n",
    "#   if numTotalSongs > 1:\n",
    "#     seed = sum([ord(char) for char in fName])\n",
    "#     random.seed(seed)\n",
    "#     file_nams = random.sample(fileNames[:170], numTotalSongs-1)\n",
    "#     for name in file_nams:\n",
    "#       fs, data1 = wavfile.read(inpathTrain + name)\n",
    "#       all_data.append(data1)\n",
    "\n",
    "#   concatenated_data = np.concatenate(all_data)\n",
    "#   if concatenated_data.ndim > 1:\n",
    "#     mono_data = np.mean(concatenated_data, axis=1, dtype='int16')\n",
    "#   else:\n",
    "#     mono_data = concatenated_data\n",
    "\n",
    "#   return mono_data.astype('int16')\n",
    "\n",
    "def loadSong(fName, numTotalSongs = 1, percentage_of_song = 1):\n",
    "  fs, data = wavfile.read(inpathTrain + fName)\n",
    "\n",
    "  if numTotalSongs > 1:\n",
    "      seed = sum(ord(char) for char in fName)\n",
    "      random.seed(seed)\n",
    "      file_nams = random.sample(fileNames[:170], numTotalSongs - 1)\n",
    "      for name in file_nams:\n",
    "          fs, data1 = wavfile.read(inpathTrain + name)\n",
    "          data = np.concatenate((data, data1), axis=0)\n",
    "  \n",
    "  if data.ndim > 1:\n",
    "    mono_data = np.mean(data, axis=1, dtype='int16')\n",
    "  else:\n",
    "    mono_data = data\n",
    "\n",
    "  return mono_data.astype('int16')\n",
    "\n",
    "\n",
    "\n",
    "def loadSongCut(fName, silence_prob = 0, numTotalSongs = 1, percentage_of_song = 1):\n",
    "  data = loadSong(fName, numTotalSongs, percentage_of_song)\n",
    "  data = cut(data, snippitLength)\n",
    "\n",
    "  # Replace rows with silence based on silence_prob\n",
    "  if silence_prob!=0:\n",
    "    num_rows = data.shape[0]\n",
    "    num_silence_rows = int(num_rows * silence_prob)\n",
    "    silence_rows = np.zeros((num_silence_rows, data.shape[1]), dtype='int16')\n",
    "    data[:num_silence_rows, :] = silence_rows\n",
    "\n",
    "  scaler[fName] = MinMaxScaler()\n",
    "  #data = quadratic_scaler(data, 5)\n",
    "  data = scaler[fName].fit_transform(data)\n",
    "\n",
    "\n",
    "  # takes very long\n",
    "  # rng = np.random.default_rng()\n",
    "  # rng.shuffle(data)\n",
    "  # if percentage_of_song != 1:\n",
    "  #   index = int(data.shape[0]*percentage_of_song)\n",
    "  #   data = data[:index]\n",
    "  Xt, Xv = train_test_split(data, test_size=0.3, random_state=42)\n",
    "  if percentage_of_song != 1:\n",
    "    index_t = int(len(Xt)*percentage_of_song)\n",
    "    index_v = int(len(Xv)*percentage_of_song)\n",
    "    Xt = Xt[:index_t]\n",
    "    Xv = Xv[:index_v]\n",
    "  return Xt, Xv\n",
    "\n",
    "\n",
    "\n",
    "def snipLoss(y_true, y_pred):\n",
    "  snipWeight = tf.convert_to_tensor([int(np.cosh(x)) for x in range(-5, 5, snippitLength)], dtype='float32')\n",
    "  squared_difference = math_ops.squared_difference(y_true, y_pred)\n",
    "  loss = math_ops.Mul(x = squared_difference, y = snipWeight)\n",
    "  loss = math_ops.log1p(loss)\n",
    "  return loss\n",
    "\n",
    "\n",
    "\n",
    "def si_snr(original, estimate):\n",
    "  # original and estimate are tensors of shape (batch_size, time_steps)\n",
    "  # compute the dot product of original and estimate along the time axis\n",
    "  dot = tf.reduce_sum(original * estimate, axis=-1, keepdims=True)\n",
    "  denominator = tf.reduce_sum(original ** 2, axis=-1, keepdims=True)\n",
    "  # compute the scaled target\n",
    "  scaled_target = dot * original / denominator\n",
    "  # compute the noise\n",
    "  e_noise = estimate - scaled_target\n",
    "  # compute the SI-SNR in decibels\n",
    "  si_snr = 10 * tf.math.log(tf.reduce_sum(scaled_target ** 2, axis=-1) / tf.reduce_sum(e_noise ** 2, axis=-1)) / tf.math.log(10.0)\n",
    "  # return the SI-SNR tensor of shape (batch_size,)\n",
    "  return si_snr\n",
    "\n",
    "\n",
    "\n",
    "def si_snr_std(original, estimate):\n",
    "  dot = np.sum(original * estimate, axis=-1, keepdims=True)\n",
    "  # compute the energy of target along the time axis\n",
    "  denominator = np.sum(original ** 2, axis=-1, keepdims=True)\n",
    "  # compute the scaled target\n",
    "  scaled_target = dot * original / denominator\n",
    "  # compute the noise\n",
    "  e_noise = estimate - scaled_target\n",
    "  # compute the SI-SNR in decibels\n",
    "  si_snr = 10 * np.log10(np.sum(scaled_target ** 2, axis=-1) / np.sum(e_noise ** 2, axis=-1))\n",
    "  # return the SI-SNR array of shape (batch_size,)\n",
    "  return si_snr\n",
    "\n",
    "# numTotalSongs = 17\n",
    "# percentage_of_song = float(1/numTotalSongs)\n",
    "# Xt, Xv = loadSongCut(fileNames[0], numTotalSongs = numTotalSongs, percentage_of_song = percentage_of_song)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################\n",
    "#####  plot history\n",
    "\n",
    "def plot_loss(ax, network_history):\n",
    "    loss = np.concatenate([network_history[key].history['loss'] for key in network_history.keys()])\n",
    "    val_loss = np.concatenate([network_history[key].history['val_loss'] for key in network_history.keys()])\n",
    "\n",
    "    ax.set_xlabel('Epochs')\n",
    "    ax.set_ylabel('Loss')\n",
    "    ax.set_title('Loss')\n",
    "    ax.plot(loss, label='Training')\n",
    "    ax.plot(val_loss, label='Validation')\n",
    "    ax.legend()\n",
    "\n",
    "def plot_si_snr(ax, network_history):\n",
    "    si_snr = np.concatenate([network_history[key].history['si_snr'] for key in network_history.keys()])\n",
    "    val_si_snr = np.concatenate([network_history[key].history['val_si_snr'] for key in network_history.keys()])\n",
    "\n",
    "    ax.set_xlabel('Epochs')\n",
    "    ax.set_ylabel('SI_SNR')\n",
    "    ax.set_title('SI-SNR')\n",
    "    ax.plot(si_snr, label='Training')\n",
    "    ax.plot(val_si_snr, label='Validation')\n",
    "    ax.legend()\n",
    "\n",
    "def plot_history(network_history, name):\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(12, 6), sharex=True, sharey=False)\n",
    "\n",
    "    plot_loss(ax[0], network_history)\n",
    "    plot_si_snr(ax[1], network_history)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(name)\n",
    "    # plt.show()\n",
    "    plt.clf()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################\n",
    "#####  buildModel (Hyperparameter grid search)\n",
    "\n",
    "def buildModel(compression_ratio = 0.5, numDense = 1, numConv = 8, numConvLayer = 0, loss_fct = snipLoss, use_bias = False, learning_rate = 0.001):\n",
    "  \n",
    "  latentSize = int(compression_ratio*snippitLength)\n",
    "\n",
    "  # keep tensorflow from allocating more memory as it currently needs\n",
    "  physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "  for i in physical_devices:\n",
    "      tf.config.experimental.set_memory_growth(i, True)\n",
    "  tf.device('/device:GPU:0')\n",
    "\n",
    "  input = Input(shape=(snippitLength,1))\n",
    "  x = input\n",
    "\n",
    "  # Convolutional part of encoder\n",
    "  for i in range(numConvLayer):\n",
    "    x = Conv1D(numConv, 5, activation='relu', padding='same')(x)\n",
    "    x = MaxPooling1D(2, padding = 'same')(x)\n",
    "\n",
    "  convShape = x.shape\n",
    "  # calculate flatten dimension\n",
    "  flsize = 1\n",
    "  for i in x.shape:\n",
    "    if(i != None):\n",
    "      flsize*= i\n",
    "  x = Flatten()(x)\n",
    "\n",
    "  # Dense part of encoder\n",
    "  denses = [int(i) for i in np.linspace(flsize, latentSize, numDense+1)]\n",
    "  print(denses)\n",
    "  print(flsize)\n",
    "  print(latentSize)\n",
    "  for i in denses[1:]:\n",
    "    x = Dense(i, activation='relu', use_bias=use_bias)(x)\n",
    "    \n",
    "  encoded = x\n",
    "\n",
    "  # Dense part of decoder\n",
    "  x = encoded\n",
    "  for i in denses[::-1][1:]:\n",
    "    if(numConvLayer == 0 and i == snippitLength):\n",
    "      x = Dense(i, activation='sigmoid')(x)\n",
    "    else:\n",
    "      x = Dense(i, activation='relu', use_bias=use_bias)(x)\n",
    "\n",
    "  if(numConvLayer == 0):\n",
    "    decoded = x\n",
    "\n",
    "  x = Reshape(convShape[1:])(x)\n",
    "\n",
    "  # Convolutional part of decoder\n",
    "  for i in range(numConvLayer):\n",
    "    x = Conv1D(numConv,5, activation='relu', padding='same')(x)\n",
    "    x = UpSampling1D(2)(x)\n",
    "  if(numConvLayer != 0):\n",
    "    decoded = Conv1D(1,5, activation='sigmoid', padding='same')(x)\n",
    "\n",
    "  autoencoder = Model(input, decoded)\n",
    "  autoencoder = Model(input, Flatten()(decoded))\n",
    "\n",
    "  autoencoder.compile(optimizer=Adam(learning_rate=learning_rate), loss=loss_fct, metrics=[si_snr])\n",
    "  \n",
    "  print(f'current model: ratio={compression_ratio},numDense={numDense},numConv={numConv},numConvLayer={numConvLayer}')\n",
    "  autoencoder.summary()\n",
    "  return autoencoder\n",
    "\n",
    "get_custom_objects()['snipLoss'] = snipLoss\n",
    "get_custom_objects()['si_snr'] = si_snr\n",
    "\n",
    "# testing:\n",
    "# Xt, Xv = loadSongCut('1727_schubert_op114_2.wav')\n",
    "# buildModel(numDense=1).fit(Xt[:2], Xt[:2],\n",
    "#             epochs=1,\n",
    "#             batch_size=512,\n",
    "#             shuffle=True,\n",
    "\n",
    "#             validation_data=(Xv[:2], Xv[:2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################\n",
    "#####  evaluate Songs\n",
    "\n",
    "def evaluateTestSongs(autoencoder, num = 0):\n",
    "  test_evaluated = []\n",
    "  if num!=0:\n",
    "    numTestSongs = num\n",
    "  print(f'evaluating {numTestSongs} test songs')\n",
    "  for songname in tqdm(reversed(fileNames[-numTestSongs:])):\n",
    "      orig = loadSong(songname)\n",
    "      origSnip = cut(orig, snippitLength)\n",
    "      orig = np.concatenate(origSnip)\n",
    "      \n",
    "      if(songname in scaler.keys()):\n",
    "        scaler_Example = scaler[songname]\n",
    "        origSnip_transformed = scaler_Example.transform(origSnip)\n",
    "      else:\n",
    "        scaler_Example = MinMaxScaler()\n",
    "        origSnip_transformed = scaler_Example.fit_transform(origSnip)\n",
    "\n",
    "      # autoencode song\n",
    "      a = autoencoder.predict(origSnip_transformed)\n",
    "      a = a.reshape(-1, snippitLength)\n",
    "      XpredSnip = scaler_Example.inverse_transform(a)\n",
    "      estimate_uncorr = np.concatenate(XpredSnip).astype('int16')\n",
    "\n",
    "\n",
    "      silence = np.zeros((1,snippitLength), dtype = 'int16')\n",
    "      a = scaler_Example.transform(silence)\n",
    "      a = autoencoder.predict(a)\n",
    "      a = a.reshape(-1, snippitLength)\n",
    "      Xsilence = scaler_Example.inverse_transform(a)[0]\n",
    "\n",
    "      # remove noise generated by silence\n",
    "      XpredSnip_minussilence = np.array(XpredSnip) - Xsilence\n",
    "      Xpred = np.concatenate(XpredSnip_minussilence).astype('int16')\n",
    "\n",
    "      test_loss, test_si_snr_uncorr = autoencoder.evaluate(origSnip_transformed, origSnip_transformed, verbose=2)\n",
    "      \n",
    "      test_si_snr_corrected = si_snr_std(orig, Xpred)\n",
    "      output_wav_name = f'{songname}_SNR={test_si_snr_corrected:.1f}.wav'\n",
    "      wavfile.write(f'{output_folder}original_{songname}', samplerate, orig)\n",
    "      wavfile.write(output_folder + output_wav_name, samplerate, Xpred)\n",
    "      print(f\"Test song predicted and saved: {output_wav_name}\")\n",
    "\n",
    "      test_evaluated.append([songname, test_loss, test_si_snr_uncorr, test_si_snr_corrected])\n",
    "  return test_evaluated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################\n",
    "#####  waveform plots\n",
    "#####  predict test song and save it\n",
    "\n",
    "def plotWave(autoencoder, name, compression_ratio, Test_Song = None):\n",
    "  if Test_Song == None:\n",
    "    Test_Song = '1752_sy_sps14.wav'\n",
    "    # Test_Song = fileNames[-1]\n",
    "  # exampleSong = name\n",
    "  # exampleSong = '1727_schubert_op114_2.wav'\n",
    "  orig = loadSong(Test_Song)\n",
    "  origSnip = cut(orig, snippitLength)\n",
    "  orig = np.concatenate(origSnip)\n",
    "\n",
    "  if(Test_Song in scaler.keys()):\n",
    "    scaler_Example = scaler[Test_Song]\n",
    "    origSnip_transformed = scaler_Example.transform(origSnip)\n",
    "  else:\n",
    "    scaler_Example = MinMaxScaler()\n",
    "    origSnip_transformed = scaler_Example.fit_transform(origSnip)\n",
    "\n",
    "  # autoencode song\n",
    "  a = autoencoder.predict(origSnip_transformed)\n",
    "  a = a.reshape(-1, snippitLength)\n",
    "  XpredSnip = scaler_Example.inverse_transform(a)\n",
    "  estimate_uncorr = np.concatenate(XpredSnip).astype('int16')\n",
    "\n",
    "  silence = np.zeros((1, snippitLength), dtype = 'int16')\n",
    "  a = scaler_Example.transform(silence)\n",
    "  a = autoencoder.predict(a)\n",
    "  a = a.reshape(-1, snippitLength)\n",
    "  Xsilence = scaler_Example.inverse_transform(a)[0]\n",
    "\n",
    "  # remove noise generated by silence\n",
    "  # XpredSnip_minussilence = [i-Xsilence for i in XpredSnip]\n",
    "  XpredSnip_minussilence = np.array(XpredSnip) - Xsilence\n",
    "  Xpred = np.concatenate(XpredSnip_minussilence).astype('int16')\n",
    "  estimate_corr = Xpred\n",
    "\n",
    "  # test_loss, test_si_snr = autoencoder.evaluate(origSnip_transformed, origSnip_transformed)\n",
    "  si_snr_uncorr = si_snr_std(orig, estimate_uncorr)\n",
    "  print(f'ucorrected SI-SNR = {si_snr_uncorr} dB')\n",
    "\n",
    "  si_snr_corr = si_snr_std(orig, estimate_corr)\n",
    "  print(f'corrected SI-SNR = {si_snr_corr} dB')\n",
    "\n",
    "\n",
    "  # output_wav_name = f'snln={snippitLength}_cmpr={compression_ratio:.1f}_loss={loss_fct.__name__}_SNR={testwav_si_snr:.1f}.wav'\n",
    "  output_wav_name = f'{Test_Song}_{compression_ratio:.1f}_SNR={si_snr_corr:.1f}.wav'\n",
    "  wavfile.write(f'{output_folder}original_{Test_Song}', samplerate, orig)\n",
    "  wavfile.write(f'{output_folder}UNCORR_{output_wav_name}', samplerate, Xpred)\n",
    "  wavfile.write(output_folder + output_wav_name, samplerate, Xpred)\n",
    "  print(f\"Test song predicted and saved: {output_wav_name}\")\n",
    "\n",
    "\n",
    "  ###### plots\n",
    "  plt.plot(orig, linewidth = 0.1)\n",
    "  plt.plot(orig-Xpred, linewidth = 0.1)\n",
    "  plt.savefig(name + \"whole.pdf\")\n",
    "  plt.clf()\n",
    "  ####################################\n",
    "  #####  see difference in waveform detailed\n",
    "  nrows = 2\n",
    "  ncols = 6\n",
    "  snips = [0, 2000, 3000, 4000, 5000, 6000, 7000, 8000, 9000, 10000, 11000, 12000]\n",
    "\n",
    "  fig, ax = plt.subplots(nrows, ncols, figsize=(6*ncols, 6*nrows), sharey = True, sharex = True)\n",
    "  s = 0\n",
    "  for i in range(nrows):\n",
    "    for j in range(ncols):\n",
    "      ax[i][j].plot(origSnip[snips[s]], linewidth = 0.5, c = 'b')\n",
    "      ax[i][j].plot(XpredSnip_minussilence[snips[s]], linewidth = 0.5, c = 'r')\n",
    "      s +=1\n",
    "  plt.savefig(name + \"snip_corrected.pdf\")\n",
    "  plt.clf()\n",
    "\n",
    "  fig, ax = plt.subplots(nrows, ncols, figsize=(6*ncols, 6*nrows), sharey = True, sharex = True)\n",
    "  s = 0\n",
    "  for i in range(nrows):\n",
    "    for j in range(ncols):\n",
    "      ax[i][j].plot(origSnip[snips[s]], linewidth = 0.5, c = 'b')\n",
    "      ax[i][j].plot(XpredSnip[snips[s]], linewidth = 0.5, c = 'r')\n",
    "      # ax[i][j].plot(XpredSnip_minussilence[snips[s]], linewidth = 0.5, c = 'r')\n",
    "      s +=1\n",
    "  plt.savefig(name + \"snip_notcorrected.pdf\")\n",
    "  plt.clf()\n",
    "\n",
    "# model_save_path = output_folder + 'train_compression_rates/' + f'model_train_1_96_3.keras' #100\n",
    "# autoencoder = tf.keras.models.load_model(model_save_path)\n",
    "# plotWave(autoencoder, f'{output_folder}train_compression_rates/model_0.2_1_96_3wave_', 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_histories_from_csv(file_path):\n",
    "    histories = {}\n",
    "    with open(file_path, 'r') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        next(reader)  # Skip the header row\n",
    "        for row in reader:\n",
    "            filename = row[0]\n",
    "            loss = float(row[1])\n",
    "            val_loss = float(row[2])\n",
    "            histories[filename] = {'loss': [loss], 'val_loss': [val_loss]}\n",
    "    return histories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths\n",
    "# drive.mount('/content/drive')\n",
    "# inpathTrain = \"/content/drive/MyDrive/Machine Learning/Autoencoder/train_data/\"\n",
    "# inpathOut = \"/content/drive/MyDrive/Machine Learning/Autoencoder/output/\"\n",
    "inpathTrain = \"songs/wav/\"\n",
    "output_folder = \"output/Versuch_new/\"\n",
    "fileNames = os.listdir(inpathTrain)\n",
    "random.seed(42)\n",
    "fileNames = random.sample(fileNames, len(fileNames))\n",
    "\n",
    "hyperparamsearch_folder = output_folder + 'hyperparamsearch'\n",
    "train_compression_rates_folder = output_folder + 'train_compression_rates'\n",
    "\n",
    "if not os.path.exists(hyperparamsearch_folder):\n",
    "    os.mkdir(hyperparamsearch_folder)\n",
    "\n",
    "if not os.path.exists(train_compression_rates_folder):\n",
    "    os.mkdir(train_compression_rates_folder)\n",
    "\n",
    "scaler = {}\n",
    "\n",
    "# global variables\n",
    "samplerate = 44_100\n",
    "snippitLength = 64\n",
    "\n",
    "loss_fct = snipLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_space = {'compression_ratio' : [0.1, 0.2, 0.3, 0.4],\n",
    "               'numDense' : [3, 4, 5, 6],\n",
    "               'numConv' : [8, 16],\n",
    "               'numConvLayer' : [0, 1, 2]}\n",
    "\n",
    "\n",
    "\n",
    "# param_space = {'compression_ratio' : np.linspace(0.1,0.9,2),\n",
    "#                'numDense' : [2, 3],\n",
    "#                'numConv' : [8, 6],\n",
    "#                'numConvLayer' : np.linspace(0.1,0.2,2)} # small 1\n",
    "# param_space = {'compression_ratio' : [0.1, 0.2],\n",
    "#                'numDense' : [3,4],\n",
    "#                'numConv' : [4, 8],\n",
    "#                'numConvLayer' : [0, 1]} # small 2\n",
    "\n",
    "\n",
    "param_space = {'compression_ratio' : [0.2],\n",
    "               'numDense' : [2, 3, 4, 5, 6],\n",
    "               'numConv' : [8, 16, 24, 32],\n",
    "               'numConvLayer' : [0, 1, 2, 3, 4]}\n",
    "\n",
    "param_space = {'compression_ratio' : [0.2],\n",
    "               'numDense' : [2, 3, 4],\n",
    "               'numConv' : [32, 64],  # 128 too much memory need\n",
    "               'numConvLayer' : [1, 2]}\n",
    "\n",
    "param_space = {'compression_ratio' : [0.2],\n",
    "               'numDense' : [1],\n",
    "               'numConv' : [32, 64],  # 128 too much memory need\n",
    "               'numConvLayer' : [1, 2]}\n",
    "\n",
    "\n",
    "param_space = {'compression_ratio' : [0.2],\n",
    "               'numDense' : [1],\n",
    "               'numConv' : [64, 96, 128],\n",
    "               'numConvLayer' : [4, 6]}\n",
    "\n",
    "\n",
    "# param_space = {'compression_ratio' : [0.2],\n",
    "#                'numDense' : [5, 6],\n",
    "#                'numConv' : [24, 32],\n",
    "#                'numConvLayer' : [ 3, 4]}\n",
    "\n",
    "#### test #####\n",
    "param_space = {'compression_ratio' : [0.2],\n",
    "               'numDense' : [1],\n",
    "               'numConv' : [64, 96, 128],\n",
    "               'numConvLayer' : [1, 2, 3]}\n",
    "\n",
    "\n",
    "\n",
    "value_combis = itertools.product(*[v for v in param_space.values()])\n",
    "param_combis = []\n",
    "for combi in value_combis:\n",
    "  param_combi = {key: value for key, value in zip(param_space.keys(), combi)}\n",
    "  if param_combi['numConvLayer'] == 0:\n",
    "    param_combi['numConv'] = 0\n",
    "  param_combis.append(param_combi)\n",
    "\n",
    "# param_combis = [\n",
    "#  {'compression_ratio': 0.2, 'numDense': 1, 'numConv': 32, 'numConvLayer': 2}, #47.8\n",
    "#  {'compression_ratio': 0.2, 'numDense': 1, 'numConv': 32, 'numConvLayer': 3}, #47.8\n",
    "#  {'compression_ratio': 0.2, 'numDense': 1, 'numConv': 64, 'numConvLayer': 2}, #47.8\n",
    "#  {'compression_ratio': 0.2, 'numDense': 1, 'numConv': 64, 'numConvLayer': 3}, #47.8\n",
    "#  {'compression_ratio': 0.2, 'numDense': 1, 'numConv': 96, 'numConvLayer': 2}, # 48.2\n",
    "#  {'compression_ratio': 0.2, 'numDense': 1, 'numConv': 96, 'numConvLayer': 3}, # 48.2\n",
    "#  {'compression_ratio': 0.2, 'numDense': 1, 'numConv': 128, 'numConvLayer': 2}, # 48.5\n",
    "#  {'compression_ratio': 0.2, 'numDense': 1, 'numConv': 128, 'numConvLayer': 3}, # 48.5\n",
    "#  {'compression_ratio': 0.2, 'numDense': 1, 'numConv': 224, 'numConvLayer': 2}, # 49.6\n",
    "#  {'compression_ratio': 0.2, 'numDense': 1, 'numConv': 224, 'numConvLayer': 3}] # 49.6\n",
    "\n",
    "#  {'compression_ratio': 0.2, 'numDense': 1, 'numConv': 96, 'numConvLayer': 2}, # 48.2\n",
    "#  {'compression_ratio': 0.2, 'numDense': 1, 'numConv': 96, 'numConvLayer': 2}, # 48.2\n",
    "#  {'compression_ratio': 0.2, 'numDense': 1, 'numConv': 96, 'numConvLayer': 2}, # 48.2\n",
    "#  {'compression_ratio': 0.2, 'numDense': 1, 'numConv': 96, 'numConvLayer': 3},\n",
    "#  {'compression_ratio': 0.2, 'numDense': 1, 'numConv': 96, 'numConvLayer': 3},\n",
    "#  {'compression_ratio': 0.2, 'numDense': 1, 'numConv': 96, 'numConvLayer': 3},\n",
    "# param_combis = [\n",
    "#  {'compression_ratio': 0.2, 'numDense': 1, 'numConv': 96, 'numConvLayer': 4},\n",
    "#  {'compression_ratio': 0.2, 'numDense': 1, 'numConv': 96, 'numConvLayer': 5},\n",
    "#  {'compression_ratio': 0.2, 'numDense': 1, 'numConv': 96, 'numConvLayer': 6}]\n",
    "\n",
    "batch_size = 4096\n",
    "batch_size = 128\n",
    "batch_size = 512\n",
    "\n",
    "numHyperEpochs = 1\n",
    "\n",
    "numTotalSongs = 17\n",
    "percentage_of_song = float(1/(numTotalSongs))\n",
    "numHyperTrainSongs = 120\n",
    "numHyperTrainSongs = 170\n",
    "\n",
    "numTotalSongs = 2\n",
    "percentage_of_song = float(1/(numTotalSongs))\n",
    "numHyperEpochs = 1\n",
    "numHyperTrainSongs = 2\n",
    "# numHyperTrainSongs = 170\n",
    "# numHyperEpochs = 10\n",
    "\n",
    "# param_combis = param_combis[5:]\n",
    "time_per_combi = 2.6\n",
    "print(f'estimated time = {time_per_combi*len(param_combis)/60:.1f} h ({len(param_combis)} sets)')\n",
    "# param_combis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################\n",
    "#####  Hyperparameter grid search\n",
    "import stopwatch as sw\n",
    "\n",
    "t = sw.stopwatch(title='gridsearch', time_unit='s')\n",
    "\n",
    "# Load existing results from the JSON file if it exists\n",
    "existing_results = []\n",
    "existing_file_path = output_folder + 'hyperparamsearch/' + 'searchResults_170songs_3epochs.json'\n",
    "existing_file_path = output_folder + 'hyperparamsearch/' + 'searchResults_170songs_1epochs_final.json'\n",
    "existing_file_path = output_folder + 'hyperparamsearch/' + 'searchResults_new.json'\n",
    "if os.path.exists(existing_file_path):\n",
    "    with open(existing_file_path, 'r') as file:\n",
    "        existing_results = json.load(file)\n",
    "\n",
    "search_results = []\n",
    "model_save_path = output_folder + 'hyperparamsearch/' + f'model.keras'\n",
    "if os.path.exists(model_save_path):\n",
    "    os.remove(model_save_path)\n",
    "\n",
    "for hyperParamSet in tqdm(param_combis):\n",
    "  autoencoder = buildModel(hyperParamSet['compression_ratio'],\n",
    "                           hyperParamSet['numDense'],\n",
    "                           hyperParamSet['numConv'],\n",
    "                           hyperParamSet['numConvLayer'])\n",
    "\n",
    "  histories = {}\n",
    "  t.task('hyperparam')\n",
    "  for idx, filename_train in tqdm(enumerate(fileNames[:numHyperTrainSongs])):\n",
    "    Xt, Xv = loadSongCut(filename_train, numTotalSongs = numTotalSongs, percentage_of_song = percentage_of_song)\n",
    "    histories[filename_train] = autoencoder.fit(Xt, Xt,\n",
    "                epochs=numHyperEpochs,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=True,\n",
    "                validation_data=(Xv, Xv))\n",
    "    del Xt\n",
    "    del Xv\n",
    "    if (idx % int(5/numHyperEpochs) == 0) and (idx != 0):\n",
    "      autoencoder.save(model_save_path)\n",
    "      del autoencoder\n",
    "      autoencoder = tf.keras.models.load_model(model_save_path)\n",
    "  t.stop()\n",
    "  del autoencoder\n",
    "  tf.keras.backend.clear_session()\n",
    "\n",
    "  pdfname = f'HyperParOpt, compression_ratio= {hyperParamSet[\"compression_ratio\"]:.1f}, numDense= {hyperParamSet[\"numDense\"]}, numConvLayer= {hyperParamSet[\"numConvLayer\"]}, numConv= {hyperParamSet[\"numConv\"]}.pdf'\n",
    "  plot_history(histories, output_folder + 'hyperparamsearch/' + pdfname)\n",
    "\n",
    "  loss = []\n",
    "  val_loss = []\n",
    "  train_si_snr = []\n",
    "  val_si_snr = []\n",
    "  for key in histories.keys():\n",
    "    loss.append(histories[key].history['loss'])\n",
    "    val_loss.append(histories[key].history['val_loss'])\n",
    "    train_si_snr.append(histories[key].history['si_snr'])\n",
    "    val_si_snr.append(histories[key].history['val_si_snr'])\n",
    "  loss         = np.concatenate(loss)\n",
    "  val_loss     = np.concatenate(val_loss)\n",
    "  train_si_snr = np.concatenate(train_si_snr)\n",
    "  val_si_snr   = np.concatenate(val_si_snr)\n",
    "\n",
    "  best_val_epoch    = np.argmax(val_si_snr)\n",
    "  best_val_si_snr   = np.max(val_si_snr)\n",
    "  best_val_loss     = np.min(val_loss)\n",
    "  best_train_si_snr = np.max(train_si_snr)\n",
    "  best_train_loss   = np.min(loss)\n",
    "\n",
    "  search_results.append({\n",
    "    **hyperParamSet,\n",
    "    'best_val_epoch': best_val_epoch,\n",
    "    'best_val_si_snr': best_val_si_snr,\n",
    "    'best_val_loss': best_val_loss,\n",
    "    'best_train_si_snr': best_train_si_snr,\n",
    "    'best_train_loss': best_train_loss\n",
    "  })\n",
    "\n",
    "\n",
    "  latest_results = [{k: int(v) if isinstance(v, np.int64) else v for k, v in d.items()} for d in search_results]\n",
    "\n",
    "  # Merge existing results and latest results\n",
    "  all_results = existing_results + latest_results\n",
    "\n",
    "  # Write all results to the JSON file\n",
    "  with open(existing_file_path, 'w') as file:\n",
    "      json.dump(all_results, file, indent='')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter plot\n",
    "from plot_hyperparameter import *\n",
    "hyperparameter_Plot(all_results, output_folder, 'result_new')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train best model\n",
    "\n",
    "# parSet_sum = {}\n",
    "# for item in search_results:\n",
    "#     numDense = item['numDense']\n",
    "#     numConv = item['numConv']\n",
    "#     numConvLayer = item['numConvLayer']\n",
    "#     best_val_si_snr = item['best_val_si_snr']\n",
    "# \n",
    "#     key = (numDense, numConv, numConvLayer)\n",
    "#     if key in parSet_sum:\n",
    "#        parSet_sum[key] += best_val_si_snr\n",
    "#     else:\n",
    "#         parSet_sum[key] = best_val_si_snr\n",
    "# \n",
    "# keys = [k for k in parSet_sum.keys()]\n",
    "# si_snr_sum = [parSet_sum[k] for k in keys]\n",
    "# bestParSet = keys[np.argmax(si_snr_sum)]\n",
    "# print(f'best set : {bestParSet}')\n",
    "\n",
    "\n",
    "# search_results_json = output_folder + 'hyperparamsearch/' + 'searchResults.json'\n",
    "# search_results_json = 'output/Versuch1_11.07.2023/searchResults.json'\n",
    "# search_results_json = 'output/Versuch3_13.07.2023/hyperparamsearch/searchResults_170songs_5epochs_final.json'\n",
    "# with open(search_results_json, 'r') as file:\n",
    "#     search_results = json.load(file)\n",
    "\n",
    "# compression_rates = np.linspace(0.1,0.9,9)\n",
    "compression_rates = [0.2]\n",
    "# silence_prob = 0.01\n",
    "\n",
    "\n",
    "numTotalSongs = 17\n",
    "percentage_of_song = float(1/(numTotalSongs))\n",
    "total_num_songs = len(fileNames)\n",
    "numTopoTrainSongs = int(((total_num_songs*0.7)+1))  # 170\n",
    "numTopoTrainSongs = 50\n",
    "numTopoTrainSongs = 2\n",
    "numTopoEpochs = 1\n",
    "\n",
    "numTestSongs = int(total_num_songs*0.3)\n",
    "\n",
    "batch_size = 64\n",
    "learning_rate = 0.00008 #no\n",
    "\n",
    "\n",
    "model_save_path = output_folder + 'train_compression_rates/' + f'model_train_1_96_3_zweite.keras' #100\n",
    "histories_save_path = output_folder + 'train_compression_rates/' + 'histories_train_1_96_3.csv'\n",
    "def lr_schedule(epoch):\n",
    "  return learning_rate\n",
    "# autoencoder = tf.keras.models.load_model(model_save_path)\n",
    "\n",
    "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "for c in compression_rates:\n",
    "  # autoencoder = buildModel(c, bestParSet[0],bestParSet[1],bestParSet[2], learning_rate = learning_rate)\n",
    "  autoencoder = buildModel(c, 1, 96, 3, learning_rate = learning_rate)\n",
    "  # autoencoder = tf.keras.models.load_model(model_save_path)\n",
    "\n",
    "  histories = {}\n",
    " \n",
    "  for idx, filename_train in tqdm(enumerate(fileNames[0:numTopoTrainSongs:])):\n",
    "    Xt, Xv = loadSongCut(filename_train, numTotalSongs = numTotalSongs, percentage_of_song = percentage_of_song)\n",
    "    histories[filename_train] = autoencoder.fit(Xt, Xt,\n",
    "                epochs=numTopoEpochs,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=True,\n",
    "                validation_data=(Xv, Xv),\n",
    "                callbacks=[lr_scheduler])\n",
    "    del Xt\n",
    "    del Xv\n",
    "    save_period = int(9/numHyperEpochs)\n",
    "    save_period = 1\n",
    "    if (idx % save_period == 0) and (idx != 0):\n",
    "      autoencoder.save(model_save_path)\n",
    "      del autoencoder\n",
    "      autoencoder = tf.keras.models.load_model(model_save_path)\n",
    "\n",
    "      # with open(histories_save_path, 'w', newline='') as csvfile:\n",
    "      #     writer = csv.writer(csvfile)\n",
    "      #     writer.writerow(['filename', 'loss', 'val_loss'])\n",
    "      #     for key, value in histories.items():\n",
    "      #         writer.writerow([key, value.history['loss'][0], value.history['val_loss'][0]], value.history['loss'][0])\n",
    "  tf.keras.backend.clear_session()\n",
    "  autoencoder.save(f'{output_folder}train_compression_rates/modelCompressionRate:{c:.1f}.keras')\n",
    "  \n",
    "  pdfname = f'BestSet, compression_ratio ={c:.1f}.pdf'\n",
    "  # histories = read_histories_from_csv(histories_save_path)\n",
    "  plot_history(histories, f'{output_folder}train_compression_rates/{pdfname}')\n",
    "  plotWave(autoencoder, f'{output_folder}train_compression_rates/modelCompressionRate:{c:.1f}wave_', c)\n",
    "\n",
    "  testPerformance = evaluateTestSongs(autoencoder, num = numTestSongs)\n",
    "\n",
    "  with open(f'{output_folder}train_compression_rates/modelCompressionRate:{c:.1f}Performance.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "\n",
    "    for row in testPerformance:\n",
    "        writer.writerow(row)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
