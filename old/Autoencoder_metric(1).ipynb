{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":312,"status":"ok","timestamp":1689112172443,"user":{"displayName":"Thasa X","userId":"04839537513707422983"},"user_tz":-120},"id":"syn0nt6E1bOO"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import os\n","from tensorflow.keras.layers import Input, Dense,Conv1D, MaxPooling1D, UpSampling1D, Flatten, Reshape\n","from tensorflow.keras.models import Model\n","from tensorflow.keras import backend as K\n","from tensorflow.python.ops import math_ops\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import MinMaxScaler\n","import tensorflow as tf\n","from tqdm import tqdm\n","from scipy.io import wavfile\n","# from google.colab import drive\n","from tensorflow.keras import regularizers\n","# from google.colab import drive"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":266,"status":"ok","timestamp":1689112112219,"user":{"displayName":"Thasa X","userId":"04839537513707422983"},"user_tz":-120},"id":"GqBX39TFv-Rz"},"outputs":[],"source":["def cut(arr, length):\n","  idx = len(arr)%length\n","  out = []\n","  while(idx+length <= len(arr)):\n","    out.append(arr[idx:idx+length])\n","    idx += length\n","  return np.array(out)\n","\n","def loadSong(fName):\n","  fs, data = wavfile.read(inpathTrain + fName)\n","  if data.ndim > 1:\n","    mono_data = np.mean(data, axis=1)\n","  else:\n","    mono_data = data\n","\n","  return mono_data.astype('int16')\n","\n","def loadSongCut(fName):\n","  data = loadSong(fName)\n","  data = cut(data, snippitLength)\n","  scaler[fName] = MinMaxScaler()\n","  #data = quadratic_scaler(data, 5)\n","  data = scaler[fName].fit_transform(data)\n","\n","  return train_test_split(data, test_size=0.3, random_state=42)\n","\n","def digital_decibel(x):\n","  if (x>0):\n","    decibels = 1/ (10 * np.log10(x/1))\n","  else:\n","    decibels = 1 / (-1*  10 * np.log10(-x/1))\n","  return decibels\n","\n","def quadratic_scaler(x, n):\n","  v = [i**n for i in x]\n","  return v"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1689112113489,"user":{"displayName":"Thasa X","userId":"04839537513707422983"},"user_tz":-120},"id":"KoAFMHZoABRx"},"outputs":[],"source":["def snipLoss(y_true, y_pred):\n","  snipWeight = tf.convert_to_tensor([int(np.cosh(x)) for x in range(-5, 5, snippitLength)], dtype='float32')\n","\n","  loss = math_ops.squared_difference(y_true,y_pred)\n","  loss = math_ops.Mul(x = loss,y = snipWeight)\n","  loss = math_ops.log1p(loss)\n","  return loss\n","\n","def si_snr2(y_true, y_pred):\n","  # Remove extra dimensions\n","  y_true = tf.squeeze(y_true)\n","  y_pred = tf.squeeze(y_pred)\n","\n","  # Compute the scaling factor\n","  scale = tf.reduce_sum(y_true * y_pred) / tf.reduce_sum(y_pred * y_pred)\n","\n","  # Compute the estimated source and the target source\n","  est_source = scale * y_pred\n","  target_source = y_true\n","\n","  # Compute the noise source\n","  noise_source = est_source - target_source\n","\n","  # Compute the SI-SNR\n","  numerator = tf.reduce_sum(target_source * est_source, axis=-1)\n","  denominator = tf.reduce_sum(noise_source * noise_source, axis=-1)\n","  si_snr = 10 * tf.math.log(numerator / denominator + 1e-8) / tf.math.log(10.0)\n","\n","  # Return the average SI-SNR\n","  return tf.reduce_mean(si_snr)\n","\n","def si_snr(target, estimate):\n","  # target and estimate are tensors of shape (batch_size, time_steps)\n","  # compute the dot product of target and estimate along the time axis\n","  dot = tf.reduce_sum(target * estimate, axis=-1, keepdims=True)\n","  # compute the energy of target along the time axis\n","  energy = tf.reduce_sum(target ** 2, axis=-1, keepdims=True)\n","  # compute the scaled target\n","  scaled_target = dot * target / energy\n","  # compute the noise\n","  noise = estimate - scaled_target\n","  # compute the SI-SNR in decibels\n","  si_snr = 10 * tf.math.log(tf.reduce_sum(scaled_target ** 2, axis=-1) / tf.reduce_sum(noise ** 2, axis=-1)) / tf.math.log(10.0)\n","  # return the SI-SNR tensor of shape (batch_size,)\n","  return si_snr"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4029,"status":"ok","timestamp":1689112280618,"user":{"displayName":"Thasa X","userId":"04839537513707422983"},"user_tz":-120},"id":"YqamE_X1B0On","outputId":"1e058a44-8385-4e82-db5a-f15abc0c90ea"},"outputs":[],"source":["# paths\n","# drive.mount('/content/drive')\n","# inpathTrain = \"/content/drive/MyDrive/Machine Learning/Autoencoder/train_data/\"\n","# inpathOut = \"/content/drive/MyDrive/Machine Learning/Autoencoder/output/\"\n","inpathTrain = \"/mnt/e/data/SynologyDrive/Uni/mSem02/Machine Learning/Project_Audio_Autoencoder/musicnet_midis/BOT/Mixdown/output/\"\n","inpathOut =   \"/home/martin/martin_user_data/jupyter_notebooks/autoencoder_ml/output/\"\n","fileNames = os.listdir(inpathTrain)\n","scaler = {}\n","\n","# global variables\n","samplerate = 44_100\n","snippitLength = 64\n","\n","loss_fct = snipLoss\n","\n","total_num_songs = len(fileNames)\n","TrainSongs = 0.25\n","numTrainSongs = int(total_num_songs*0.7*TrainSongs)\n","numTrainSongs = 20\n","numTestSongs = int(total_num_songs*0.3*TrainSongs)\n","numTestSongs = 3\n","numHyperTrainSongs = 5\n","\n","\n","# Hyperparameters\n","compression_ratio = 0.5\n","latentSize = int(compression_ratio*snippitLength)\n","# latentSize = 50\n","numDense = 3\n","numConvLayer = 2\n","numConv = 8\n","\n","\n","output_wav_name = f'snln={snippitLength}_cmpr={compression_ratio}_loss={loss_fct}_songs={numTrainSongs}'"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":306,"status":"ok","timestamp":1689112243940,"user":{"displayName":"Thasa X","userId":"04839537513707422983"},"user_tz":-120},"id":"7Cl3Ffyg7uxx"},"outputs":[],"source":["####################################\n","#####  plot history\n","\n","def plot_loss(ax, network_history):\n","    loss = np.concatenate([network_history[key].history['loss'] for key in network_history.keys()])\n","    val_loss = np.concatenate([network_history[key].history['val_loss'] for key in network_history.keys()])\n","\n","    ax.set_xlabel('Epochs')\n","    ax.set_ylabel('Loss')\n","    ax.set_title('Loss')\n","    ax.plot(loss, label='Training')\n","    ax.plot(val_loss, label='Validation')\n","    ax.legend()\n","\n","def plot_si_snr(ax, network_history):\n","    si_snr = np.concatenate([network_history[key].history['si_snr'] for key in network_history.keys()])\n","    val_si_snr = np.concatenate([network_history[key].history['val_si_snr'] for key in network_history.keys()])\n","\n","    ax.set_xlabel('Epochs')\n","    ax.set_ylabel('SI_SNR')\n","    ax.set_title('SI-SNR')\n","    ax.plot(si_snr, label='Training')\n","    ax.plot(val_si_snr, label='Validation')\n","    ax.legend()\n","\n","def plot_history(network_history, name):\n","    fig, ax = plt.subplots(1, 2, figsize=(12, 6), sharex=True, sharey=False)\n","\n","    plot_loss(ax[0], network_history)\n","    plot_si_snr(ax[1], network_history)\n","\n","    plt.tight_layout()\n","    plt.savefig(name)\n","    # plt.show()\n","    plt.clf()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":315,"status":"ok","timestamp":1689112246220,"user":{"displayName":"Thasa X","userId":"04839537513707422983"},"user_tz":-120},"id":"v5Bv1lsxEPSL"},"outputs":[],"source":["def buildModel(compression_ratio = 0.5, numDense = 3, numConv = 8,numConvLayer = 2, loss_fct = snipLoss):\n","\n","  latentSize = int(compression_ratio*snippitLength)\n","  physical_devices = tf.config.experimental.list_physical_devices('GPU')\n","  for i in physical_devices:\n","      tf.config.experimental.set_memory_growth(i, True)\n","  tf.device('/device:GPU:0')\n","\n","  input = Input(shape=(snippitLength,1))\n","  x = input\n","\n","  # Convolutional part of encoder\n","  for i in range(numConvLayer):\n","    x = Conv1D(numConv, 5, activation='relu', padding='same')(x)\n","    x = MaxPooling1D(2, padding = 'same')(x)\n","\n","  convShape = x.shape\n","  # calculate flatten dimension\n","  flsize = 1\n","  for i in x.shape:\n","    if(i != None):\n","      flsize*= i\n","\n","  print(f'{latentSize} {flsize}')\n","\n","  x = Flatten()(x)\n","  convShape2 = x.shape\n","\n","\n","  # Dense part of encoder\n","  denses = [int(i) for i in np.linspace(flsize, latentSize, numDense)]\n","  for i in denses[1:]:\n","    x = Dense(i, activation='relu')(x)\n","\n","  encoded = x\n","\n","\n","  # Dense part of decoder\n","  x = encoded\n","  for i in denses[::-1][1:]:\n","    if(numConvLayer == 0 and i == snippitLength):\n","      x = Dense(i, activation='sigmoid')(x)\n","    else:\n","      x = Dense(i, activation='relu')(x)\n","\n","  if(numConvLayer == 0):\n","    decoded = x\n","\n","  x = Reshape(convShape[1:])(x)\n","\n","  # Convolutional part of decoder\n","  for i in range(numConvLayer):\n","    x = Conv1D(numConv,5, activation='relu', padding='same')(x)\n","    x = UpSampling1D(2)(x)\n","  if(numConvLayer != 0):\n","    decoded = Conv1D(1,5, activation='sigmoid', padding='same')(x)\n","\n","  autoencoder = Model(input, decoded)\n","  autoencoder = Model(input, Flatten()(decoded))\n","\n","  autoencoder.compile(optimizer='adam', loss=loss_fct, metrics=[si_snr])\n","\n","  autoencoder.summary()\n","  return autoencoder"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":402,"status":"ok","timestamp":1689112249189,"user":{"displayName":"Thasa X","userId":"04839537513707422983"},"user_tz":-120},"id":"EcSaNyxC66NZ","outputId":"1f75acec-4823-494a-d349-049550b31de8"},"outputs":[],"source":["param_space = {'compression_ratio' : np.linspace(0.1,0.9,5),\n","               'numDense' : [int(i) for i in np.linspace(2, 4, 3)],\n","               'numConv' : [8,16],\n","               'numConvLayer' : np.linspace(0,2, 3, dtype = int)}\n","\n","# param_space = {'compression_ratio' : np.linspace(0.1,0.9,5),\n","#                'numDense' : [2],\n","#                'numConv' : [8],\n","#                'numConvLayer' : [0, 1]}\n","\n","\n","\n","import itertools\n","value_combis = itertools.product(*[v for v in param_space.values()])\n","param_combis = []\n","for combi in value_combis:\n","  param_combis.append({key: value for key, value in zip(param_space.keys(), combi)})\n","len(param_combis)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Iu7RTlE668_-"},"outputs":[],"source":["#Hyperparameter grid search\n","search_results = []\n","\n","\n","for hyperParamSet in tqdm(param_combis):\n","  autoencoder = buildModel(hyperParamSet['compression_ratio'],\n","                           hyperParamSet['numDense'],\n","                           hyperParamSet['numConv'],\n","                           hyperParamSet['numConvLayer'])\n","\n","  autoencoder = buildModel()\n","  histories = {}\n","  for filename_train in tqdm(fileNames[:numHyperTrainSongs]):\n","    Xt, Xv = loadSongCut(filename_train)\n","    histories[filename_train] = autoencoder.fit(Xt, Xt,\n","                epochs=5,\n","                batch_size=2**10,\n","                shuffle=True,\n","                validation_data=(Xv, Xv))\n","\n","  plot_history(histories, f'HyperParOpt, compression_ratio ={hyperParamSet[\"compression_ratio\"]:.1f}, numDense ={hyperParamSet[\"numDense\"]}, numConv = {hyperParamSet[\"numConv\"]}, numConvLayer = {hyperParamSet[\"numConvLayer\"]}.pdf')\n","\n","  loss = []\n","  val_loss = []\n","  for key in histories.keys():\n","    loss.append(histories[key].history['loss'])\n","    val_loss.append(histories[key].history['val_loss'])\n","\n","  loss = np.concatenate(loss)\n","  val_loss = np.concatenate(val_loss)\n","\n","\n","  train_si_snr = []\n","  val_si_snr = []\n","  for key in histories.keys():\n","    train_si_snr.append(histories[key].history['si_snr'])\n","    val_si_snr.append(histories[key].history['val_si_snr'])\n","\n","  train_si_snr = np.concatenate(train_si_snr)\n","  val_si_snr = np.concatenate(val_si_snr)\n","\n","  best_val_epoch    = np.argmax(val_si_snr)\n","  best_val_si_snr      = np.max(val_si_snr)\n","  best_val_loss     = np.min(val_loss)\n","\n","  best_train_si_snr      = np.max(train_si_snr)\n","  best_train_loss     = np.min(loss)\n","\n","  search_results.append({\n","        **hyperParamSet,\n","        'best_val_epoch': best_val_epoch,\n","        'best_val_si_snr': best_val_si_snr,\n","        'best_val_loss': best_val_loss,\n","        'best_train_si_snr': best_train_si_snr,\n","        'best_train_loss': best_train_loss\n","    })\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zRzwd5jJ7Gj-"},"outputs":[],"source":["import json\n","\n","results = [{k: int(v) if isinstance(v, np.int64) else v for k, v in d.items()} for d in search_results]\n","\n","with open('searchResults.json', 'w') as file:\n","    json.dump(results, file, indent = '')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fe4Pbw928oVb"},"outputs":[],"source":["####################################\n","#####  evaluate test songs\n","\n","# numTestSongs = 2\n","\n","def evaluateTestSongs(autoencoder):\n","  index = 0\n","  test_evaluated = []\n","  for songname in tqdm(reversed(fileNames[-numTestSongs:])):\n","      # songname = '1727_schubert_op114_2.wav'\n","      print(songname)\n","      song = loadSong(songname)\n","      # wavfile.write(inpathOut + songname + '.wav', samplerate, song)\n","      songSnip = cut(song, snippitLength)\n","\n","      songSnip_transformed = MinMaxScaler().fit_transform(songSnip)\n","      test_loss, test_si_snr = autoencoder.evaluate(songSnip_transformed, songSnip_transformed, verbose=2)\n","\n","      test_evaluated.append([songname, test_loss, test_si_snr])\n","\n","\n","  return test_evaluated\n","\n","# test_loss, test_si_snr = autoencoder.evaluate(songSnips_transformed, songSnips_transformed, verbose=2)\n","# print(test_evaluated)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_1XeEUQ-90bk"},"outputs":[],"source":["####################################\n","#####  predict unknown song\n","\n","def plotWave(autoencoder, name, compression_ratio):\n","  exampleSong = fileNames[-1]\n","  # exampleSong = '1727_schubert_op114_2.wav'\n","  orig = loadSong(exampleSong)\n","  origSnip = cut(orig, snippitLength)\n","  orig = np.concatenate(origSnip)\n","\n","  if(exampleSong in scaler.keys()):\n","    scaler_Example = scaler[exampleSong]\n","    origSnip_transformed = scaler_Example.transform(origSnip)\n","  else:\n","    scaler_Example = MinMaxScaler()\n","    origSnip_transformed = scaler_Example.fit_transform(origSnip)\n","\n","  # autoencode song\n","  a = autoencoder.predict(origSnip_transformed)\n","  a = a.reshape(-1, snippitLength)\n","  XpredSnip = scaler_Example.inverse_transform(a)\n","\n","  silence = np.zeros((1,snippitLength), dtype = 'int16')\n","  a = scaler_Example.transform(silence)\n","  a = autoencoder.predict(a)\n","  a = a.reshape(-1, snippitLength)\n","  Xsilence = scaler_Example.inverse_transform(a)[0]\n","\n","  # remove noise generated by silence\n","  XpredSnip = [i-Xsilence for i in XpredSnip]\n","  Xpred = np.concatenate(XpredSnip).astype('int16')\n","\n","  test_loss, test_si_snr = autoencoder.evaluate(origSnip_transformed, origSnip_transformed)\n","\n","  output_wav_name = f'snln={snippitLength}_cmpr={compression_ratio:.1f}_loss={loss_fct}_songs={numTrainSongs}_SNR={test_si_snr:.1f}.wav'\n","  wavfile.write(inpathOut + output_wav_name, samplerate, Xpred)\n","  print(f\"file saved: {output_wav_name}\")\n","\n","  plt.plot(orig, linewidth = 0.1)\n","  plt.plot(orig-Xpred, linewidth = 0.1)\n","  plt.savefig(name + \"whole.pdf\")\n","  plt.clf()\n","  ####################################\n","  #####  see difference in waveform detailed\n","  nrows = 2\n","  ncols = 6\n","  snips = [0, 2000, 3000, 4000, 5000, 6000, 7000, 8000, 9000, 10000, 11000, 12000]\n","\n","  fig, ax = plt.subplots(nrows, ncols, figsize=(6*ncols, 6*nrows), sharey = True, sharex = True)\n","  s = 0\n","  for i in range(nrows):\n","    for j in range(ncols):\n","      ax[i][j].plot(origSnip[snips[s]], linewidth = 0.5, c = 'b')\n","      ax[i][j].plot(XpredSnip[snips[s]], linewidth = 0.5, c = 'r')\n","      s +=1\n","  plt.savefig(name + \"snip.pdf\")\n","  plt.clf()\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["param_space = {'compression_ratio' : np.linspace(0.1,0.9,5),\n","               'numDense' : [2],\n","               'numConv' : [8],\n","               'numConvLayer' : [0, 1]}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_XQYjrwo7IcC"},"outputs":[],"source":["# Find best topology across different compression ratios and build/train 10 models across compression ratios with that topology\n","import csv\n","parSet_sum = {}\n","\n","for item in search_results:\n","    numDense = item['numDense']\n","    numConv = item['numConv']\n","    numConvLayer = item['numConvLayer']\n","    best_val_si_snr = item['best_val_si_snr']\n","\n","    key = (numDense, numConv, numConvLayer)\n","    if key in parSet_sum:\n","       parSet_sum[key] += best_val_si_snr\n","    else:\n","        parSet_sum[key] = best_val_si_snr\n","\n","keys = [k for k in parSet_sum.keys()]\n","si_snr_sum = [parSet_sum[k] for k in keys]\n","bestParSet = keys[np.argmax(si_snr_sum)]\n","\n","\n","compression_rates = np.linspace(0.1,0.9,9)\n","for c in compression_rates:\n","  autoencoder = buildModel(c, bestParSet[0],bestParSet[1],bestParSet[2])\n","\n","  histories = {}\n","  numTrainSongs = 20\n","  for filename_train in tqdm(fileNames[:numTrainSongs]):\n","    Xt, Xv = loadSongCut(filename_train)\n","    histories[filename_train] = autoencoder.fit(Xt, Xt,\n","                epochs=5,\n","                batch_size=2**10,\n","                shuffle=True,\n","                validation_data=(Xv, Xv))\n","  plot_history(histories, f'BestSet, compression_ratio ={c:.1f}.pdf')\n","\n","  autoencoder.save(outputpdf + 'modelCompressionRate:{c:.1f}.keras')\n","  testPerformance = evaluateTestSongs(autoencoder)\n","  with open(f'modelCompressionRate:{c:.1f}Performance.csv', 'w', newline='') as file:\n","    # Create a CSV writer\n","    writer = csv.writer(file)\n","\n","    # Write the data row by row\n","    for row in testPerformance:\n","        writer.writerow(row)\n","  plotWave(autoencoder, f'modelCompressionRate:{c:.1f}wave_', c)\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pgKZrEl6CGnU","tags":["fit"]},"outputs":[],"source":["# #numTrainSongs =\n","# batch_size = 2**10\n","# epochs = 5\n","# histories = {}\n","# for fn in tqdm(fileNames[:numTrainSongs]):\n","#   Xt, Xv = loadSongCut(fn)\n","#   Xt = np.array(Xt)\n","#   Xv = np.array(Xv)\n","#   histories[fn] = autoencoder.fit(Xt, Xt,\n","#                 epochs=epochs,\n","#                 batch_size=batch_size,\n","#                 shuffle=True,\n","#                 validation_data=(Xv, Xv))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mF1zOJ_7LbFJ","outputId":"ae27c216-bdd0-40c6-b7d4-86fe012bd010"},"outputs":[],"source":["# plot_history(histories, 'test')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CX_f64w71m5P","outputId":"dbf23d75-9bc2-47a5-bec1-c05f785a4583"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.17"}},"nbformat":4,"nbformat_minor":0}
